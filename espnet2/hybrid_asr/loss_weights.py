import torch

gold_spk={
    "M":"p256",
    "F":"p301"
}

spk_to_netidx={
    "p225":0,
    "p226":1,
    "p227":2,
    "p228":3,
    "p229":4,
    "p230":5,
    "p231":6,
    "p232":7,
    "p233":8,
    "p234":9,
    "p236":10,
    "p237":11,
    "p238":12,
    "p239":13,
    "p240":14,
    "p241":15,
    "p243":16,
    "p244":17,
    "p245":18,
    "p246":19,
    "p247":20,
    "p248":21,
    "p249":22,
    "p250":23,
    "p251":24,
    "p252":25,
    "p253":26,
    "p254":27,
    "p255":28,
    "p256":29,
    "p257":30,
    "p258":31,
    "p259":32,
    "p260":33,
    "p261":34,
    "p262":35,
    "p263":36,
    "p264":37,
    "p265":38,
    "p266":39,
    "p267":40,
    "p268":41,
    "p269":42,
    "p270":43,
    "p271":44,
    "p272":45,
    "p273":46,
    "p274":47,
    "p275":48,
    "p276":49,
    "p277":50,
    "p278":51,
    "p279":52,
    "p280":53,
    "p281":54,
    "p282":55,
    "p283":56,
    "p284":57,
    "p285":58,
    "p286":59,
    "p287":60,
    "p288":61,
    "p292":62,
    "p293":63,
    "p294":64,
    "p295":65,
    "p297":66,
    "p298":67,
    "p299":68,
    "p300":69,
    "p301":70,
    "p302":71,
    "p303":72,
    "p304":73,
    "p305":74,
    "p306":75,
    "p307":76,
    "p308":77,
    "p310":78,
    "p311":79,
    "p312":80,
    "p313":81,
    "p314":82,
    "p316":83,
    "p317":84,
    "p318":85,
    "p323":86,
    "p326":87,
    "p329":88,
    "p330":89,
    "p333":90,
    "p334":91,
    "p335":92,
    "p336":93,
    "p339":94,
    "p340":95,
    "p341":96,
    "p343":97,
    "p345":98,
    "p347":99,
    "p351":100,
    "p360":101,
    "p361":102,
    "p362":103,
    "p363":104,
    "p364":105,
    "p374":106,
    "p376":107,
}

spk_to_gender={
    "p225": "F", 
    "p226": "M", 
    "p227": "M", 
    "p228": "F", 
    "p229": "F", 
    "p230": "F", 
    "p231": "F", 
    "p232": "M", 
    "p233": "F", 
    "p234": "F", 
    "p236": "F", 
    "p237": "M", 
    "p238": "F", 
    "p239": "F", 
    "p240": "F", 
    "p241": "M", 
    "p243": "M", 
    "p244": "F", 
    "p245": "M", 
    "p246": "M", 
    "p247": "M", 
    "p248": "F", 
    "p249": "F", 
    "p250": "F", 
    "p251": "M", 
    "p252": "M", 
    "p253": "F", 
    "p254": "M", 
    "p255": "M", 
    "p256": "M", 
    "p257": "F", 
    "p258": "M", 
    "p259": "M", 
    "p260": "M", 
    "p261": "F", 
    "p262": "F", 
    "p263": "M", 
    "p264": "F", 
    "p265": "F", 
    "p266": "F", 
    "p267": "F", 
    "p268": "F", 
    "p269": "F", 
    "p270": "M", 
    "p271": "M", 
    "p272": "M", 
    "p273": "M", 
    "p274": "M", 
    "p275": "M", 
    "p276": "F", 
    "p277": "F", 
    "p278": "M", 
    "p279": "M", 
    "p280": "F", 
    "p281": "M", 
    "p282": "F", 
    "p283": "F", 
    "p284": "M", 
    "p285": "M", 
    "p286": "M", 
    "p287": "M", 
    "p288": "F", 
    "p292": "M", 
    "p293": "F", 
    "p294": "F", 
    "p295": "F", 
    "p297": "F", 
    "p298": "M", 
    "p299": "F", 
    "p300": "F", 
    "p301": "F", 
    "p302": "M", 
    "p303": "F", 
    "p304": "M", 
    "p305": "F", 
    "p306": "F", 
    "p307": "F", 
    "p308": "F", 
    "p310": "F", 
    "p311": "M", 
    "p312": "F", 
    "p313": "F", 
    "p314": "F", 
    "p315": "M", 
    "p316": "M", 
    "p317": "F", 
    "p318": "F", 
    "p323": "F", 
    "p326": "M", 
    "p329": "F", 
    "p330": "F", 
    "p333": "F", 
    "p334": "M", 
    "p335": "F", 
    "p336": "F", 
    "p339": "F", 
    "p340": "F", 
    "p341": "F", 
    "p343": "F", 
    "p345": "M", 
    "p347": "M", 
    "p351": "F", 
    "p360": "M", 
    "p361": "F", 
    "p362": "F", 
    "p363": "M", 
    "p364": "M", 
    "p374": "M", 
    "p376": "M", 
    "s5":"F", 
}


idx_to_vq_min=[
    71,
    58,
    59,
    230,
    450,
    170,
    470,
    223,
    308,
    41,
    449,
    161,
    200,
    93,
    290,
    455,
    341,
    115,
    350,
    399,
    224,
    202,
    459,
    442,
    56,
    490,
    447,
    334,
    386,
    292,
    126,
    497,
    287,
    394,
    359,
    502,
    452,
    66,
    397,
    139,
    361,
    241,
    239,
    357,
    250,
    242,
    152,
    340,
    29,
    280,
    358,
    231,
    256,
    210,
    14,
    351,
    124,
    62,
    57,
    8,
    229,
    203,
    155,
    323,
    503,
    360,
    326,
    464,
    325,
    19,
    267,
    412,
    38,
    389,
    298,
    286,
    150,
    103,
    402,
    303,
    100,
    262,
    40,
    377,
    339,
    272,
    387,
    27,
    393,
    174,
    81,
    432,
    252,
    9,
    419,
    396,
    141,
    183,
    0,
    79,
    26,
    436,
    138,
    479,
    279,
    205,
    466,
    478,
    410,
    77,
    133,
    353,
    95,
]

idx_to_vq_max=[
    71,
    58,
    59,
    230,
    450,
    170,
    470,
    223,
    308,
    41,
    449,
    200,
    161,
    93,
    290,
    341,
    115,
    455,
    399,
    350,
    202,
    459,
    224,
    442,
    490,
    447,
    386,
    334,
    287,
    56,
    292,
    497,
    359,
    502,
    126,
    394,
    452,
    139,
    361,
    397,
    66,
    239,
    357,
    241,
    250,
    242,
    280,
    152,
    231,
    358,
    29,
    340,
    210,
    124,
    62,
    8,
    351,
    256,
    155,
    14,
    57,
    19,
    203,
    229,
    464,
    503,
    326,
    267,
    360,
    323,
    412,
    325,
    150,
    38,
    298,
    389,
    103,
    262,
    303,
    286,
    40,
    272,
    402,
    377,
    100,
    387,
    339,
    27,
    393,
    174,
    81,
    432,
    252,
    9,
    419,
    396,
    141,
    183,
    0,
    79,
    436,
    26,
    138,
    479,
    205,
    279,
    466,
    478,
    410,
    77,
    133,
    353,
    324,
    268,
    95,
    42
]

idx_to_vq = idx_to_vq_max

label_samples=[
    10888174,
    4239994,
    3764906,
    2832657,
    2355986,
    1407264,
    1465425,
    1283446,
    1309389,
    1198724,
    1076269,
    838217,
    770442,
    746908,
    675613,
    602303,
    599189,
    666542,
    564322,
    563007,
    491817,
    502596,
    573153,
    504542,
    489471,
    483707,
    443495,
    458990,
    417451,
    488922,
    395639,
    435645,
    420558,
    421057,
    454435,
    417093,
    394462,
    369486,
    370035,
    344336,
    373462,
    355162,
    337502,
    310203,
    322019,
    331866,
    280456,
    325277,
    278009,
    314916,
    314502,
    309215,
    271945,
    279476,
    268366,
    273357,
    282746,
    291919,
    277133,
    288849,
    280442,
    260575,
    278289,
    263387,
    260539,
    276940,
    265651,
    266467,
    268625,
    265779,
    259352,
    264321,
    233377,
    248513,
    249558,
    247734,
    238930,
    237114,
    237972,
    227185,
    233741,
    226440,
    230379,
    228354,
    223397,
    215155,
    218371,
    211654,
    216080,
    212899,
    208194,
    195754,
    190549,
    187444,
    177319,
    153806,
    145137,
    125141,
    124866,
    119975,
    104381,
    100514,
    84695,
    40134,
    21981,
    22182,
    8175,
    758,
    69,
    66,
    28,
    4,
    1,
    1,
    1,
    1,
]


# option 1: just give some specific weights
# normedWeights = [1 (x / sum(label_samples)) for x in label_samples]
# normedWeights[0:3] = [0.05, 0.08, 0.1]
# print('weights:', normedWeights)

# option 2: (log2(x+100)) ** 2  
normedWeights = torch.FloatTensor(label_samples)
normedWeights = torch.log2(normedWeights + 100)
normedWeights = normedWeights * normedWeights
normedWeights = 1 / (normedWeights / normedWeights.sum())
normedWeights = 100 * normedWeights / normedWeights.sum()

print('weights:', normedWeights)